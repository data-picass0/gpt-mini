# ðŸ§  MiniGPT: A Tiny Transformer-Based Language Model

A minimal implementation of a GPT-style language model in PyTorch, inspired by Andrej Karpathy's legendary "nanoGPT" series. The model is trained on a plain text file using the Transformer architecture.

## ðŸš€ Features

- Self-attention (multi-head)
- Transformer blocks (attention + feed-forward)
- Positional and token embeddings
- Cross-entropy loss optimization
- Text generation from scratch
- Clean, readable PyTorch implementation
- No external dependencies beyond PyTorch

## ðŸ“š References

- [Attention is All You Need](https://arxiv.org/abs/1706.03762)
- [Karpathy's nanoGPT](https://github.com/karpathy/nanoGPT)
